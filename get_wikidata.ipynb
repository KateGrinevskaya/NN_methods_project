{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Эта тетрадка скачивает статьи из Википедии по теме \"Лингвистика\" и создаёт таблицу sent_def.\n",
        "\n",
        "text_sent | has_def\n",
        "- | -\n",
        "текст предложения |0\n",
        "\n",
        "\n",
        "У первых предложений из статей has_def = 1, у остальных has_def = 0.\n",
        "\n",
        "sent_def.csv требует ручной проверки."
      ],
      "metadata": {
        "id": "k7ND4h82W7ST"
      },
      "id": "k7ND4h82W7ST"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bd94ed92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd94ed92",
        "outputId": "07f68668-5e00-4312-cd68-6d0d9fc452e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=39bb946e6dad31bf4e4f97e10b0fd0aa5dcf15d196b1a8470340ce405606d410\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "! pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e71d731a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e71d731a",
        "outputId": "a3bb27e9-7fb3-48aa-825a-854d64676892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c67ad916",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c67ad916",
        "outputId": "686c12f3-79a4-447f-8068-2d0bfe669c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import wikipedia\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6f82dbc0",
      "metadata": {
        "id": "6f82dbc0"
      },
      "outputs": [],
      "source": [
        "wikipedia.set_lang(\"ru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "77d2bfbc",
      "metadata": {
        "id": "77d2bfbc"
      },
      "outputs": [],
      "source": [
        "page_names = wikipedia.search(\"Лингвистика\", 450)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c40d722b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c40d722b",
        "outputId": "02310b73-6930-4a02-b4cb-8adf916c07ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.7/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        }
      ],
      "source": [
        "sent_def = pd.DataFrame()\n",
        "pages_exceptions = []\n",
        "for page_name in page_names:\n",
        "    try:\n",
        "        page_data = wikipedia.page(page_name)\n",
        "        page_text = re.split(r'== См. также ==', page_data.content)[0]\n",
        "        page_text = re.sub(r'===.*?===', '', page_text, count=0)\n",
        "        page_text = re.sub(r'==.*?==', '', page_text, count=0)\n",
        "        page_text = re.sub(r'\\n', ' ', page_text, count=0)\n",
        "        sents = sent_tokenize(page_text)\n",
        "        df = pd.DataFrame({\"text_sent\": sents, \"has_def\": 0})\n",
        "        df['has_def'][0] = 1\n",
        "        \n",
        "        sent_def = pd.DataFrame(sent_def.append(df, ignore_index=True))\n",
        "    except Exception:\n",
        "        pages_exceptions.append(page_name)\n",
        "sent_def.to_csv('sent_def.csv', sep=';')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(pages_exceptions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Z2Z1bGMPGu",
        "outputId": "39532e43-3cef-4b32-fa47-1fa7ae43639e"
      },
      "id": "M4Z2Z1bGMPGu",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "get wikidata.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}